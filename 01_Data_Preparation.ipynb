{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 1. Data Preparation for RLHF\n",
        "\n",
        "This notebook handles the download, processing, and preparation of our dataset for RLHF training. We'll be working with the Anthropic/hh-rlhf dataset to create properly formatted preference pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import random\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "Dataset: Anthropic/hh-rlhf\n",
            "Training samples: 5000\n",
            "Test samples: 1000\n",
            "Output directory: data\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables - easy to adjust for experiments\n",
        "DATASET_NAME = 'Anthropic/hh-rlhf'\n",
        "NUM_SAMPLES_TRAIN = 5000\n",
        "NUM_SAMPLES_TEST = 1000\n",
        "OUTPUT_DIR = 'data'\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Training samples: {NUM_SAMPLES_TRAIN}\")\n",
        "print(f\"Test samples: {NUM_SAMPLES_TEST}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from Hugging Face Hub...\n",
            "‚úÖ Dataset loaded successfully!\n",
            "\n",
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['chosen', 'rejected'],\n",
            "        num_rows: 160800\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['chosen', 'rejected'],\n",
            "        num_rows: 8552\n",
            "    })\n",
            "})\n",
            "\n",
            "Dataset info:\n",
            "train: 160800 examples\n",
            "test: 8552 examples\n",
            "\n",
            "Example from training set:\n",
            "Keys: ['chosen', 'rejected']\n",
            "\n",
            "Chosen text (first 200 chars): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here‚Äôs an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n",
            "\n",
            "Rejected text (first 200 chars): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here‚Äôs an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n"
          ]
        }
      ],
      "source": [
        "# Load the raw dataset from Hugging Face Hub\n",
        "print(\"Loading dataset from Hugging Face Hub...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(DATASET_NAME)\n",
        "    print(\"‚úÖ Dataset loaded successfully!\")\n",
        "    \n",
        "    print(\"\\nDataset structure:\")\n",
        "    print(raw_dataset)\n",
        "\n",
        "    print(\"\\nDataset info:\")\n",
        "    for split in raw_dataset.keys():\n",
        "        print(f\"{split}: {len(raw_dataset[split])} examples\")\n",
        "        \n",
        "    # Show an example to understand the data format\n",
        "    print(\"\\nExample from training set:\")\n",
        "    example = raw_dataset['train'][0]\n",
        "    print(f\"Keys: {list(example.keys())}\")\n",
        "    print(f\"\\nChosen text (first 200 chars): {example['chosen'][:200]}...\")\n",
        "    print(f\"\\nRejected text (first 200 chars): {example['rejected'][:200]}...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "    print(\"Please check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating sampled datasets...\n",
            "\n",
            "Sampled dataset sizes:\n",
            "Training set: 5000 examples\n",
            "Test set: 1000 examples\n",
            "\n",
            "Sampling complete!\n"
          ]
        }
      ],
      "source": [
        "# Create smaller, manageable subsets for experiments\n",
        "print(\"Creating sampled datasets...\")\n",
        "\n",
        "# Shuffle for reproducibility and sample\n",
        "train_sampled = raw_dataset['train'].shuffle(seed=42).select(range(NUM_SAMPLES_TRAIN))\n",
        "test_sampled = raw_dataset['test'].shuffle(seed=42).select(range(NUM_SAMPLES_TEST))\n",
        "\n",
        "print(f\"\\nSampled dataset sizes:\")\n",
        "print(f\"Training set: {len(train_sampled)} examples\")\n",
        "print(f\"Test set: {len(test_sampled)} examples\")\n",
        "\n",
        "print(\"\\nSampling complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing preprocessing function...\n",
            "\n",
            "Original chosen text (first 150 chars): \n",
            "\n",
            "Human: Why did cells originally combine together to create life?\n",
            "\n",
            "Assistant: Because their simple components -- chemicals -- interacted in particula...\n",
            "\n",
            "Processed:\n",
            "Prompt (first 100 chars): Human: Why did cells originally combine together to create life?...\n",
            "Chosen (first 100 chars): Because their simple components -- chemicals -- interacted in particular ways.  And because of chemi...\n",
            "Rejected (first 100 chars): Cells combine because they benefit from cooperation, since they can have less competition for resour...\n",
            "\n",
            "Preprocessing function ready!\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing logic\n",
        "def preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Preprocess a single example from the hh-rlhf dataset.\n",
        "    \n",
        "    Args:\n",
        "        example: Dict with 'chosen' and 'rejected' fields\n",
        "        \n",
        "    Returns:\n",
        "        Dict with 'prompt', 'chosen', and 'rejected' fields\n",
        "    \"\"\"\n",
        "    \n",
        "    def extract_prompt_and_response(text):\n",
        "        \"\"\"Extract prompt and response from conversation text\"\"\"\n",
        "        try:\n",
        "            # Find the last occurrence of \"\\n\\nAssistant:\"\n",
        "            assistant_marker = \"\\n\\nAssistant:\"\n",
        "            last_assistant_idx = text.rfind(assistant_marker)\n",
        "            \n",
        "            if last_assistant_idx == -1:\n",
        "                # Fallback: try \"Assistant:\" without double newlines\n",
        "                assistant_marker = \"Assistant:\"\n",
        "                last_assistant_idx = text.rfind(assistant_marker)\n",
        "                \n",
        "            if last_assistant_idx == -1:\n",
        "                # If no Assistant marker found, treat whole text as response\n",
        "                return \"\", text.strip()\n",
        "            \n",
        "            # Split into prompt and response\n",
        "            prompt = text[:last_assistant_idx].strip()\n",
        "            response = text[last_assistant_idx + len(assistant_marker):].strip()\n",
        "            \n",
        "            return prompt, response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text: {e}\")\n",
        "            return \"\", text.strip()\n",
        "    \n",
        "    # Process chosen and rejected responses\n",
        "    chosen_text = example['chosen']\n",
        "    rejected_text = example['rejected']\n",
        "    \n",
        "    # Extract prompt and responses\n",
        "    prompt_chosen, chosen_response = extract_prompt_and_response(chosen_text)\n",
        "    prompt_rejected, rejected_response = extract_prompt_and_response(rejected_text)\n",
        "    \n",
        "    # Use the chosen prompt (they should be the same)\n",
        "    prompt = prompt_chosen if prompt_chosen else prompt_rejected\n",
        "    \n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen_response,\n",
        "        'rejected': rejected_response\n",
        "    }\n",
        "\n",
        "# Test the function with an example\n",
        "print(\"Testing preprocessing function...\")\n",
        "test_example = train_sampled[0]\n",
        "processed = preprocess_example(test_example)\n",
        "\n",
        "print(f\"\\nOriginal chosen text (first 150 chars): {test_example['chosen'][:150]}...\")\n",
        "print(f\"\\nProcessed:\")\n",
        "print(f\"Prompt (first 100 chars): {processed['prompt'][:100]}...\")\n",
        "print(f\"Chosen (first 100 chars): {processed['chosen'][:100]}...\")\n",
        "print(f\"Rejected (first 100 chars): {processed['rejected'][:100]}...\")\n",
        "\n",
        "print(\"\\nPreprocessing function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying preprocessing to datasets...\n",
            "Preprocessing complete!\n",
            "\n",
            "Saving datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88064e256d8448d9a5499f2acaf4ef01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6b762a5f2d54e4484117ce0707e1324",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Data preparation complete!\n",
            "Training data saved to: data\\train_prefs.jsonl\n",
            "Test data saved to: data\\test_prefs.jsonl\n",
            "\n",
            "Dataset sizes:\n",
            "Training: 5000 examples\n",
            "Test: 1000 examples\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing and save data\n",
        "print(\"Applying preprocessing to datasets...\")\n",
        "\n",
        "# Apply preprocessing function to both datasets\n",
        "train_processed = train_sampled.map(preprocess_example)\n",
        "test_processed = test_sampled.map(preprocess_example)\n",
        "\n",
        "print(\"Preprocessing complete!\")\n",
        "\n",
        "# Define output file paths\n",
        "train_output_path = os.path.join(OUTPUT_DIR, 'train_prefs.jsonl')\n",
        "test_output_path = os.path.join(OUTPUT_DIR, 'test_prefs.jsonl')\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save processed datasets to disk\n",
        "print(f\"\\nSaving datasets...\")\n",
        "train_processed.to_json(train_output_path)\n",
        "test_processed.to_json(test_output_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Data preparation complete!\")\n",
        "print(f\"Training data saved to: {train_output_path}\")\n",
        "print(f\"Test data saved to: {test_output_path}\")\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Training: {len(train_processed)} examples\")\n",
        "print(f\"Test: {len(test_processed)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying saved data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8ddba033bb49c4a1fd6098bcee8e45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded dataset size: 5000\n",
            "\n",
            "First example from loaded dataset:\n",
            "\n",
            "Keys: ['chosen', 'rejected', 'prompt']\n",
            "\n",
            "Prompt: Human: Why did cells originally combine together to create life?...\n",
            "\n",
            "Chosen response: Because their simple components -- chemicals -- interacted in particular ways.  And because of chemical processes involving acids and bases, certain kinds of chemicals can begin to self-organize into ...\n",
            "\n",
            "Rejected response: Cells combine because they benefit from cooperation, since they can have less competition for resources by working together....\n",
            "\n",
            "‚úÖ Data format verification passed!\n",
            "‚úÖ Your dataset is ready for RLHF training!\n",
            "\n",
            "üéâ Data preparation and verification complete!\n"
          ]
        }
      ],
      "source": [
        "# Verify saved data\n",
        "print(\"Verifying saved data...\")\n",
        "\n",
        "# Load the saved training data back\n",
        "train_output_path = os.path.join(OUTPUT_DIR, 'train_prefs.jsonl')\n",
        "loaded_dataset = load_dataset('json', data_files=train_output_path)['train']\n",
        "\n",
        "print(f\"\\nLoaded dataset size: {len(loaded_dataset)}\")\n",
        "print(f\"\\nFirst example from loaded dataset:\")\n",
        "first_example = loaded_dataset[0]\n",
        "\n",
        "print(f\"\\nKeys: {list(first_example.keys())}\")\n",
        "print(f\"\\nPrompt: {first_example['prompt'][:200]}...\")\n",
        "print(f\"\\nChosen response: {first_example['chosen'][:200]}...\")\n",
        "print(f\"\\nRejected response: {first_example['rejected'][:200]}...\")\n",
        "\n",
        "# Verify the format is correct\n",
        "required_keys = {'prompt', 'chosen', 'rejected'}\n",
        "actual_keys = set(first_example.keys())\n",
        "\n",
        "if required_keys.issubset(actual_keys):\n",
        "    print(\"\\n‚úÖ Data format verification passed!\")\n",
        "    print(\"‚úÖ Your dataset is ready for RLHF training!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Missing keys: {required_keys - actual_keys}\")\n",
        "    \n",
        "print(f\"\\nüéâ Data preparation and verification complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
