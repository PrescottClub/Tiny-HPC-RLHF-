{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 1. Data Preparation for RLHF\n",
        "\n",
        "This notebook handles the download, processing, and preparation of our dataset for RLHF training. We'll be working with the Anthropic/hh-rlhf dataset to create properly formatted preference pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import random\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "Dataset: Anthropic/hh-rlhf\n",
            "Training samples: 5000\n",
            "Test samples: 1000\n",
            "Output directory: data\n"
          ]
        }
      ],
      "source": [
        "# Configuration variables - easy to adjust for experiments\n",
        "DATASET_NAME = 'Anthropic/hh-rlhf'\n",
        "NUM_SAMPLES_TRAIN = 5000\n",
        "NUM_SAMPLES_TEST = 1000\n",
        "OUTPUT_DIR = 'data'\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Training samples: {NUM_SAMPLES_TRAIN}\")\n",
        "print(f\"Test samples: {NUM_SAMPLES_TEST}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from Hugging Face Hub...\n",
            "✅ Dataset loaded successfully!\n",
            "\n",
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['chosen', 'rejected'],\n",
            "        num_rows: 160800\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['chosen', 'rejected'],\n",
            "        num_rows: 8552\n",
            "    })\n",
            "})\n",
            "\n",
            "Dataset info:\n",
            "train: 160800 examples\n",
            "test: 8552 examples\n",
            "\n",
            "Example from training set:\n",
            "Keys: ['chosen', 'rejected']\n",
            "\n",
            "Chosen text (first 200 chars): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here’s an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n",
            "\n",
            "Rejected text (first 200 chars): \n",
            "\n",
            "Human: What are some cuss words in english?\n",
            "\n",
            "Assistant: Here’s an incomplete list.\n",
            "\n",
            "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cu...\n"
          ]
        }
      ],
      "source": [
        "# Load the raw dataset from Hugging Face Hub\n",
        "print(\"Loading dataset from Hugging Face Hub...\")\n",
        "try:\n",
        "    raw_dataset = load_dataset(DATASET_NAME)\n",
        "    print(\"✅ Dataset loaded successfully!\")\n",
        "    \n",
        "    print(\"\\nDataset structure:\")\n",
        "    print(raw_dataset)\n",
        "\n",
        "    print(\"\\nDataset info:\")\n",
        "    for split in raw_dataset.keys():\n",
        "        print(f\"{split}: {len(raw_dataset[split])} examples\")\n",
        "        \n",
        "    # Show an example to understand the data format\n",
        "    print(\"\\nExample from training set:\")\n",
        "    example = raw_dataset['train'][0]\n",
        "    print(f\"Keys: {list(example.keys())}\")\n",
        "    print(f\"\\nChosen text (first 200 chars): {example['chosen'][:200]}...\")\n",
        "    print(f\"\\nRejected text (first 200 chars): {example['rejected'][:200]}...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading dataset: {e}\")\n",
        "    print(\"Please check your internet connection and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating sampled datasets...\n",
            "\n",
            "Sampled dataset sizes:\n",
            "Training set: 5000 examples\n",
            "Test set: 1000 examples\n",
            "\n",
            "Sampling complete!\n"
          ]
        }
      ],
      "source": [
        "# Create smaller, manageable subsets for experiments\n",
        "print(\"Creating sampled datasets...\")\n",
        "\n",
        "# Shuffle for reproducibility and sample\n",
        "train_sampled = raw_dataset['train'].shuffle(seed=42).select(range(NUM_SAMPLES_TRAIN))\n",
        "test_sampled = raw_dataset['test'].shuffle(seed=42).select(range(NUM_SAMPLES_TEST))\n",
        "\n",
        "print(f\"\\nSampled dataset sizes:\")\n",
        "print(f\"Training set: {len(train_sampled)} examples\")\n",
        "print(f\"Test set: {len(test_sampled)} examples\")\n",
        "\n",
        "print(\"\\nSampling complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing preprocessing function...\n",
            "\n",
            "Original chosen text (first 150 chars): \n",
            "\n",
            "Human: Why did cells originally combine together to create life?\n",
            "\n",
            "Assistant: Because their simple components -- chemicals -- interacted in particula...\n",
            "\n",
            "Processed:\n",
            "Prompt (first 100 chars): Human: Why did cells originally combine together to create life?...\n",
            "Chosen (first 100 chars): Because their simple components -- chemicals -- interacted in particular ways.  And because of chemi...\n",
            "Rejected (first 100 chars): Cells combine because they benefit from cooperation, since they can have less competition for resour...\n",
            "\n",
            "Preprocessing function ready!\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing logic\n",
        "def preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Preprocess a single example from the hh-rlhf dataset.\n",
        "    \n",
        "    Args:\n",
        "        example: Dict with 'chosen' and 'rejected' fields\n",
        "        \n",
        "    Returns:\n",
        "        Dict with 'prompt', 'chosen', and 'rejected' fields\n",
        "    \"\"\"\n",
        "    \n",
        "    def extract_prompt_and_response(text):\n",
        "        \"\"\"Extract prompt and response from conversation text\"\"\"\n",
        "        try:\n",
        "            # Find the last occurrence of \"\\n\\nAssistant:\"\n",
        "            assistant_marker = \"\\n\\nAssistant:\"\n",
        "            last_assistant_idx = text.rfind(assistant_marker)\n",
        "            \n",
        "            if last_assistant_idx == -1:\n",
        "                # Fallback: try \"Assistant:\" without double newlines\n",
        "                assistant_marker = \"Assistant:\"\n",
        "                last_assistant_idx = text.rfind(assistant_marker)\n",
        "                \n",
        "            if last_assistant_idx == -1:\n",
        "                # If no Assistant marker found, treat whole text as response\n",
        "                return \"\", text.strip()\n",
        "            \n",
        "            # Split into prompt and response\n",
        "            prompt = text[:last_assistant_idx].strip()\n",
        "            response = text[last_assistant_idx + len(assistant_marker):].strip()\n",
        "            \n",
        "            return prompt, response\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text: {e}\")\n",
        "            return \"\", text.strip()\n",
        "    \n",
        "    # Process chosen and rejected responses\n",
        "    chosen_text = example['chosen']\n",
        "    rejected_text = example['rejected']\n",
        "    \n",
        "    # Extract prompt and responses\n",
        "    prompt_chosen, chosen_response = extract_prompt_and_response(chosen_text)\n",
        "    prompt_rejected, rejected_response = extract_prompt_and_response(rejected_text)\n",
        "    \n",
        "    # Use the chosen prompt (they should be the same)\n",
        "    prompt = prompt_chosen if prompt_chosen else prompt_rejected\n",
        "    \n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen_response,\n",
        "        'rejected': rejected_response\n",
        "    }\n",
        "\n",
        "# Test the function with an example\n",
        "print(\"Testing preprocessing function...\")\n",
        "test_example = train_sampled[0]\n",
        "processed = preprocess_example(test_example)\n",
        "\n",
        "print(f\"\\nOriginal chosen text (first 150 chars): {test_example['chosen'][:150]}...\")\n",
        "print(f\"\\nProcessed:\")\n",
        "print(f\"Prompt (first 100 chars): {processed['prompt'][:100]}...\")\n",
        "print(f\"Chosen (first 100 chars): {processed['chosen'][:100]}...\")\n",
        "print(f\"Rejected (first 100 chars): {processed['rejected'][:100]}...\")\n",
        "\n",
        "print(\"\\nPreprocessing function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying preprocessing to datasets...\n",
            "Preprocessing complete!\n",
            "\n",
            "Saving datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88064e256d8448d9a5499f2acaf4ef01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6b762a5f2d54e4484117ce0707e1324",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Data preparation complete!\n",
            "Training data saved to: data\\train_prefs.jsonl\n",
            "Test data saved to: data\\test_prefs.jsonl\n",
            "\n",
            "Dataset sizes:\n",
            "Training: 5000 examples\n",
            "Test: 1000 examples\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing and save data\n",
        "print(\"Applying preprocessing to datasets...\")\n",
        "\n",
        "# Apply preprocessing function to both datasets\n",
        "train_processed = train_sampled.map(preprocess_example)\n",
        "test_processed = test_sampled.map(preprocess_example)\n",
        "\n",
        "print(\"Preprocessing complete!\")\n",
        "\n",
        "# Define output file paths\n",
        "train_output_path = os.path.join(OUTPUT_DIR, 'train_prefs.jsonl')\n",
        "test_output_path = os.path.join(OUTPUT_DIR, 'test_prefs.jsonl')\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Save processed datasets to disk\n",
        "print(f\"\\nSaving datasets...\")\n",
        "train_processed.to_json(train_output_path)\n",
        "test_processed.to_json(test_output_path)\n",
        "\n",
        "print(f\"\\n✅ Data preparation complete!\")\n",
        "print(f\"Training data saved to: {train_output_path}\")\n",
        "print(f\"Test data saved to: {test_output_path}\")\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Training: {len(train_processed)} examples\")\n",
        "print(f\"Test: {len(test_processed)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying saved data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8ddba033bb49c4a1fd6098bcee8e45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded dataset size: 5000\n",
            "\n",
            "First example from loaded dataset:\n",
            "\n",
            "Keys: ['chosen', 'rejected', 'prompt']\n",
            "\n",
            "Prompt: Human: Why did cells originally combine together to create life?...\n",
            "\n",
            "Chosen response: Because their simple components -- chemicals -- interacted in particular ways.  And because of chemical processes involving acids and bases, certain kinds of chemicals can begin to self-organize into ...\n",
            "\n",
            "Rejected response: Cells combine because they benefit from cooperation, since they can have less competition for resources by working together....\n",
            "\n",
            "✅ Data format verification passed!\n",
            "✅ Your dataset is ready for RLHF training!\n",
            "\n",
            "🎉 Data preparation and verification complete!\n"
          ]
        }
      ],
      "source": [
        "# Verify saved data\n",
        "print(\"Verifying saved data...\")\n",
        "\n",
        "# Load the saved training data back\n",
        "train_output_path = os.path.join(OUTPUT_DIR, 'train_prefs.jsonl')\n",
        "loaded_dataset = load_dataset('json', data_files=train_output_path)['train']\n",
        "\n",
        "print(f\"\\nLoaded dataset size: {len(loaded_dataset)}\")\n",
        "print(f\"\\nFirst example from loaded dataset:\")\n",
        "first_example = loaded_dataset[0]\n",
        "\n",
        "print(f\"\\nKeys: {list(first_example.keys())}\")\n",
        "print(f\"\\nPrompt: {first_example['prompt'][:200]}...\")\n",
        "print(f\"\\nChosen response: {first_example['chosen'][:200]}...\")\n",
        "print(f\"\\nRejected response: {first_example['rejected'][:200]}...\")\n",
        "\n",
        "# Verify the format is correct\n",
        "required_keys = {'prompt', 'chosen', 'rejected'}\n",
        "actual_keys = set(first_example.keys())\n",
        "\n",
        "if required_keys.issubset(actual_keys):\n",
        "    print(\"\\n✅ Data format verification passed!\")\n",
        "    print(\"✅ Your dataset is ready for RLHF training!\")\n",
        "else:\n",
        "    print(f\"\\n❌ Missing keys: {required_keys - actual_keys}\")\n",
        "    \n",
        "print(f\"\\n🎉 Data preparation and verification complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
