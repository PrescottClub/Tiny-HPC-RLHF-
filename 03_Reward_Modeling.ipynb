{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Reward Modeling with Quantization\n",
        "\n",
        "This notebook serves as an experimental platform to train multiple reward models with different quantization levels (bf16, 8-bit, 4-bit) based on our fine-tuned SFT model. We'll systematically compare the performance and memory usage of different precision levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\downloads\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "All libraries imported successfully!\n",
            "PyTorch version: 2.7.0+cu118\n",
            "Transformers version: 4.53.0\n",
            "BitsAndBytes version: 0.46.0\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "GPU Memory: 8.0 GB\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries for reward modeling\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM, \n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import RewardTrainer\n",
        "from peft import PeftModel\n",
        "import bitsandbytes as bnb\n",
        "import gc\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"BitsAndBytes version: {bnb.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up experimental configuration...\n",
            "Configuration:\n",
            "  SFT model path: ./models/sft\n",
            "  Dataset path: ./data/train_prefs.jsonl\n",
            "  Base RM output dir: ./models/rm\n",
            "  Precisions to test: ['bf16', 'int8', 'int4']\n",
            "  Shared training args: {'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'num_train_epochs': 1, 'learning_rate': 0.0002, 'logging_steps': 10, 'bf16': True, 'save_strategy': 'epoch', 'evaluation_strategy': 'no', 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': None, 'dataloader_pin_memory': False, 'gradient_checkpointing': True}\n",
            "‚úÖ Configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# Central Configuration for all reward modeling experiments\n",
        "print(\"Setting up experimental configuration...\")\n",
        "\n",
        "# Model and dataset paths\n",
        "sft_model_path = './models/sft'          # Path to our trained SFT adapters\n",
        "dataset_path = './data/train_prefs.jsonl'  # Preference dataset\n",
        "base_rm_output_dir = './models/rm'       # Base output directory for reward models\n",
        "\n",
        "# Precision levels to experiment with\n",
        "precisions_to_run = ['bf16', 'int8', 'int4']\n",
        "\n",
        "# Shared training arguments for all experiments\n",
        "shared_training_args = {\n",
        "    'per_device_train_batch_size': 4,     # Batch size per device\n",
        "    'gradient_accumulation_steps': 4,     # Effective batch size = 4*4 = 16\n",
        "    'num_train_epochs': 1,                # Number of training epochs\n",
        "    'learning_rate': 2e-4,                # Learning rate for reward model training\n",
        "    'logging_steps': 10,                  # Log every 10 steps\n",
        "    'bf16': True,                         # Use BF16 for training efficiency\n",
        "    'save_strategy': 'epoch',             # Save at the end of each epoch\n",
        "    'evaluation_strategy': 'no',          # No evaluation during training\n",
        "    'remove_unused_columns': False,       # Keep all columns\n",
        "    'push_to_hub': False,                 # Don't push to HF Hub\n",
        "    'report_to': None,                    # Disable logging to wandb/tensorboard\n",
        "    'dataloader_pin_memory': False,       # Reduce memory usage\n",
        "    'gradient_checkpointing': True,       # Trade compute for memory\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  SFT model path: {sft_model_path}\")\n",
        "print(f\"  Dataset path: {dataset_path}\")\n",
        "print(f\"  Base RM output dir: {base_rm_output_dir}\")\n",
        "print(f\"  Precisions to test: {precisions_to_run}\")\n",
        "print(f\"  Shared training args: {shared_training_args}\")\n",
        "\n",
        "# Ensure base output directory exists\n",
        "os.makedirs(base_rm_output_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing preference dataset...\n",
            "Loaded 5000 preference pairs\n",
            "‚úì Pad token set to EOS token\n",
            "Tokenizer loaded - Vocab size: 50257\n",
            "Tokenizing preference pairs...\n",
            "Tokenization complete!\n",
            "Dataset keys: ['chosen', 'rejected', 'prompt', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
            "Sample chosen length: 512\n",
            "Sample rejected length: 512\n",
            "Final dataset size: 5000 examples\n"
          ]
        }
      ],
      "source": [
        "# Load and tokenize the preference dataset\n",
        "print(\"Loading and preparing preference dataset...\")\n",
        "\n",
        "# Load the preference dataset\n",
        "preference_dataset = load_dataset('json', data_files=dataset_path)['train']\n",
        "print(f\"Loaded {len(preference_dataset)} preference pairs\")\n",
        "\n",
        "# Load tokenizer from the SFT model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
        "\n",
        "# Critical: Set pad token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úì Pad token set to EOS token\")\n",
        "\n",
        "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "def tokenize_pairs(example):\n",
        "    \"\"\"\n",
        "    Tokenize chosen and rejected responses for RewardTrainer.\n",
        "    \n",
        "    Returns dictionary with keys expected by RewardTrainer:\n",
        "    - input_ids_chosen, attention_mask_chosen\n",
        "    - input_ids_rejected, attention_mask_rejected\n",
        "    \"\"\"\n",
        "    # Format the texts for reward modeling\n",
        "    chosen_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['chosen']}\"\n",
        "    rejected_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['rejected']}\"\n",
        "    \n",
        "    # Tokenize chosen response\n",
        "    chosen_tokens = tokenizer(\n",
        "        chosen_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    # Tokenize rejected response\n",
        "    rejected_tokens = tokenizer(\n",
        "        rejected_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\", \n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"],\n",
        "        \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"],\n",
        "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"],\n",
        "        \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# Apply tokenization to the entire dataset\n",
        "print(\"Tokenizing preference pairs...\")\n",
        "tokenized_dataset = preference_dataset.map(\n",
        "    tokenize_pairs,\n",
        "    batched=False,\n",
        "    desc=\"Tokenizing preference pairs\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Dataset keys: {list(tokenized_dataset[0].keys())}\")\n",
        "print(f\"Sample chosen length: {len(tokenized_dataset[0]['input_ids_chosen'])}\")\n",
        "print(f\"Sample rejected length: {len(tokenized_dataset[0]['input_ids_rejected'])}\")\n",
        "\n",
        "# Take a subset for faster training (optional)\n",
        "# tokenized_dataset = tokenized_dataset.select(range(min(1000, len(tokenized_dataset))))\n",
        "print(f\"Final dataset size: {len(tokenized_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing reward model factory...\n",
            "Creating bf16 reward model...\n",
            "  Using BF16 precision\n",
            "  Loading PEFT adapters...\n",
            "  Creating reward model with classification head...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Test the factory function\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting reward model factory...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m test_model, test_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_reward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msft_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbf16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest model parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_model\u001b[38;5;241m.\u001b[39mnum_parameters()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Clean up test model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[4], line 81\u001b[0m, in \u001b[0;36mcreate_reward_model\u001b[1;34m(model_path, precision)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Create reward model by adding a classification head\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Creating reward model with classification head...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m reward_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't load from pretrained\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single reward score\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Copy the transformer layers from our fine-tuned model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[1;32md:\\downloads\\python\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:569\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m class_ref:\n\u001b[0;32m    568\u001b[0m         upstream_repo \u001b[38;5;241m=\u001b[39m class_ref\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 569\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupstream_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trust_remote_code\n\u001b[0;32m    578\u001b[0m \u001b[38;5;66;03m# Set the adapter kwargs\u001b[39;00m\n",
            "File \u001b[1;32md:\\downloads\\python\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:701\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message, upstream_repo)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m upstream_repo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m references custom code contained in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupstream_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be executed to correctly load the model. You can inspect the repository \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    699\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupstream_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    700\u001b[0m     )\n\u001b[1;32m--> 701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    702\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe repository \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contains custom code which must be executed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto correctly load the model. You can inspect the repository \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(model_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32m<frozen genericpath>:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
          ]
        }
      ],
      "source": [
        "# Reward Model Factory Function\n",
        "def create_reward_model(model_path, precision):\n",
        "    \"\"\"\n",
        "    Create a reward model with specified precision/quantization.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to the SFT model\n",
        "        precision: One of 'bf16', 'int8', 'int4'\n",
        "        \n",
        "    Returns:\n",
        "        reward_model: The reward model for training\n",
        "        tokenizer: Associated tokenizer\n",
        "    \"\"\"\n",
        "    print(f\"Creating {precision} reward model...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Define model loading kwargs based on precision\n",
        "    if precision == 'bf16':\n",
        "        print(\"  Using BF16 precision\")\n",
        "        model_kwargs = {\n",
        "            'torch_dtype': torch.bfloat16,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int8':\n",
        "        print(\"  Using 8-bit quantization\")\n",
        "        model_kwargs = {\n",
        "            'load_in_8bit': True,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int4':\n",
        "        print(\"  Using 4-bit QLoRA quantization\")\n",
        "        # Configure 4-bit quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",                # Use NormalFloat4 quantization\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,    # Compute in BF16\n",
        "            bnb_4bit_use_double_quant=True,           # Double quantization for better compression\n",
        "        )\n",
        "        model_kwargs = {\n",
        "            'quantization_config': bnb_config,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported precision: {precision}\")\n",
        "    \n",
        "    # Load the base model (this will be our SFT model)\n",
        "    try:\n",
        "        # First try to load as a PEFT model (if it has adapters)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            'distilgpt2',  # Base model name\n",
        "            **model_kwargs\n",
        "        )\n",
        "        \n",
        "        # Load PEFT adapters if they exist\n",
        "        if os.path.exists(os.path.join(model_path, 'adapter_config.json')):\n",
        "            print(\"  Loading PEFT adapters...\")\n",
        "            model = PeftModel.from_pretrained(base_model, model_path)\n",
        "            # Merge adapters for reward modeling\n",
        "            model = model.merge_and_unload()\n",
        "        else:\n",
        "            print(\"  No PEFT adapters found, using base model\")\n",
        "            model = base_model\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading model: {e}\")\n",
        "        print(\"  Falling back to direct loading...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            **model_kwargs\n",
        "        )\n",
        "    \n",
        "    # Create reward model by adding a classification head\n",
        "    print(\"  Creating reward model with classification head...\")\n",
        "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        None,  # Don't load from pretrained\n",
        "        config=model.config,\n",
        "        num_labels=1,  # Single reward score\n",
        "        torch_dtype=model.dtype if hasattr(model, 'dtype') else torch.bfloat16\n",
        "    )\n",
        "    \n",
        "    # Copy the transformer layers from our fine-tuned model\n",
        "    if hasattr(model, 'transformer'):\n",
        "        reward_model.transformer = model.transformer\n",
        "    elif hasattr(model, 'model'):\n",
        "        reward_model.model = model.model\n",
        "    else:\n",
        "        # Copy all non-classifier parameters\n",
        "        for name, param in model.named_parameters():\n",
        "            if hasattr(reward_model, name.split('.')[0]):\n",
        "                target = reward_model\n",
        "                for attr in name.split('.'):\n",
        "                    if hasattr(target, attr):\n",
        "                        target = getattr(target, attr)\n",
        "                    else:\n",
        "                        break\n",
        "                else:\n",
        "                    target.data = param.data\n",
        "    \n",
        "    print(f\"  ‚úÖ {precision} reward model created successfully!\")\n",
        "    return reward_model, tokenizer\n",
        "\n",
        "# Test the factory function\n",
        "print(\"Testing reward model factory...\")\n",
        "test_model, test_tokenizer = create_reward_model(sft_model_path, 'bf16')\n",
        "print(f\"Test model parameters: {test_model.num_parameters():,}\")\n",
        "\n",
        "# Clean up test model\n",
        "del test_model, test_tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Factory function test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment Execution Loop - Train Multiple Reward Models\n",
        "print(\"üöÄ Starting reward model training experiments...\")\n",
        "print(f\"Will train {len(precisions_to_run)} different precision models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Track results for summary\n",
        "training_results = {}\n",
        "\n",
        "for i, precision in enumerate(precisions_to_run):\n",
        "    print(f\"\\n{'='*20} Experiment {i+1}/{len(precisions_to_run)} {'='*20}\")\n",
        "    print(f\"--- Starting {precision.upper()} Reward Model Training ---\")\n",
        "    \n",
        "    try:\n",
        "        # Create reward model and tokenizer for current precision\n",
        "        model, tokenizer = create_reward_model(sft_model_path, precision)\n",
        "        \n",
        "        # Define output directory for this specific precision\n",
        "        output_dir = os.path.join(base_rm_output_dir, precision)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Create training arguments for this run\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            **shared_training_args  # Unpack shared arguments\n",
        "        )\n",
        "        \n",
        "        print(f\"  Output directory: {output_dir}\")\n",
        "        print(f\"  Model parameters: {model.num_parameters():,}\")\n",
        "        \n",
        "        # Initialize RewardTrainer\n",
        "        print(\"  Initializing RewardTrainer...\")\n",
        "        trainer = RewardTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            # eval_dataset=None,  # No evaluation for now\n",
        "        )\n",
        "        \n",
        "        print(f\"  Starting training for {precision} model...\")\n",
        "        # Start training\n",
        "        training_result = trainer.train()\n",
        "        \n",
        "        # Save the trained model\n",
        "        print(f\"  Saving {precision} model...\")\n",
        "        trainer.save_model()\n",
        "        \n",
        "        # Store results\n",
        "        training_results[precision] = {\n",
        "            'status': 'success',\n",
        "            'final_loss': training_result.training_loss if hasattr(training_result, 'training_loss') else 'N/A',\n",
        "            'output_dir': output_dir,\n",
        "            'model_size_mb': sum(os.path.getsize(os.path.join(output_dir, f)) \n",
        "                                for f in os.listdir(output_dir) \n",
        "                                if os.path.isfile(os.path.join(output_dir, f))) / (1024*1024)\n",
        "        }\n",
        "        \n",
        "        print(f\"  ‚úÖ {precision.upper()} model training completed successfully!\")\n",
        "        print(f\"  Final loss: {training_results[precision]['final_loss']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error training {precision} model: {str(e)}\")\n",
        "        training_results[precision] = {\n",
        "            'status': 'failed',\n",
        "            'error': str(e),\n",
        "            'output_dir': output_dir if 'output_dir' in locals() else 'N/A'\n",
        "        }\n",
        "    \n",
        "    finally:\n",
        "        # CRITICAL: Clean up GPU memory before next iteration\n",
        "        # This prevents CUDA out of memory errors when switching between quantization levels\n",
        "        print(f\"  üßπ Cleaning up GPU memory after {precision} training...\")\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'trainer' in locals():\n",
        "            del trainer\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        \n",
        "        # Force garbage collection and clear CUDA cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"  Memory cleaned up for {precision} model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ All reward model experiments completed!\")\n",
        "\n",
        "# Print summary of results\n",
        "print(\"\\nüìä TRAINING SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "for precision, result in training_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {precision.upper():6} | Status: {result['status']:8}\")\n",
        "    \n",
        "    if result['status'] == 'success':\n",
        "        print(f\"        | Loss: {result['final_loss']}\")\n",
        "        print(f\"        | Size: {result['model_size_mb']:.1f} MB\")\n",
        "        print(f\"        | Path: {result['output_dir']}\")\n",
        "    else:\n",
        "        print(f\"        | Error: {result['error']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"üéØ Your reward models are ready in: {base_rm_output_dir}\")\n",
        "print(\"Next step: Use these models for PPO training or inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
