{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Reward Modeling with Quantization\n",
        "\n",
        "This notebook serves as an experimental platform to train multiple reward models with different quantization levels (bf16, 8-bit, 4-bit) based on our fine-tuned SFT model. We'll systematically compare the performance and memory usage of different precision levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\downloads\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "All libraries imported successfully!\n",
            "PyTorch version: 2.7.0+cu118\n",
            "Transformers version: 4.53.0\n",
            "BitsAndBytes version: 0.46.0\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "GPU Memory: 8.0 GB\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries for reward modeling\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM, \n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import RewardTrainer\n",
        "from peft import PeftModel\n",
        "import bitsandbytes as bnb\n",
        "import gc\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"BitsAndBytes version: {bnb.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up experimental configuration...\n",
            "Configuration:\n",
            "  SFT model path: ./models/sft\n",
            "  Dataset path: ./data/train_prefs.jsonl\n",
            "  Base RM output dir: ./models/rm\n",
            "  Precisions to test: ['bf16', 'int8', 'int4']\n",
            "  Shared training args: {'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'num_train_epochs': 1, 'learning_rate': 0.0002, 'logging_steps': 10, 'bf16': True, 'save_strategy': 'epoch', 'eval_strategy': 'no', 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': None, 'dataloader_pin_memory': False, 'gradient_checkpointing': True}\n",
            "✅ Configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# Central Configuration for all reward modeling experiments\n",
        "print(\"Setting up experimental configuration...\")\n",
        "\n",
        "# Model and dataset paths\n",
        "sft_model_path = './models/sft'          # Path to our trained SFT adapters\n",
        "dataset_path = './data/train_prefs.jsonl'  # Preference dataset\n",
        "base_rm_output_dir = './models/rm'       # Base output directory for reward models\n",
        "\n",
        "# Precision levels to experiment with\n",
        "precisions_to_run = ['bf16', 'int8', 'int4']\n",
        "\n",
        "# Shared training arguments for all experiments\n",
        "shared_training_args = {\n",
        "    'per_device_train_batch_size': 4,     # Batch size per device\n",
        "    'gradient_accumulation_steps': 4,     # Effective batch size = 4*4 = 16\n",
        "    'num_train_epochs': 1,                # Number of training epochs\n",
        "    'learning_rate': 2e-4,                # Learning rate for reward model training\n",
        "    'logging_steps': 10,                  # Log every 10 steps\n",
        "    'bf16': True,                         # Use BF16 for training efficiency\n",
        "    'save_strategy': 'epoch',             # Save at the end of each epoch\n",
        "    'eval_strategy': 'no',                 # 修复：使用正确的参数名\n",
        "    'remove_unused_columns': False,       # Keep all columns\n",
        "    'push_to_hub': False,                 # Don't push to HF Hub\n",
        "    'report_to': None,                    # Disable logging to wandb/tensorboard\n",
        "    'dataloader_pin_memory': False,       # Reduce memory usage\n",
        "    'gradient_checkpointing': True,       # Trade compute for memory\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  SFT model path: {sft_model_path}\")\n",
        "print(f\"  Dataset path: {dataset_path}\")\n",
        "print(f\"  Base RM output dir: {base_rm_output_dir}\")\n",
        "print(f\"  Precisions to test: {precisions_to_run}\")\n",
        "print(f\"  Shared training args: {shared_training_args}\")\n",
        "\n",
        "# Ensure base output directory exists\n",
        "os.makedirs(base_rm_output_dir, exist_ok=True)\n",
        "print(f\"✅ Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preparing preference dataset...\n",
            "Loaded 5000 preference pairs\n",
            "✓ Pad token set to EOS token\n",
            "Tokenizer loaded - Vocab size: 50257\n",
            "Tokenizing preference pairs...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28da543ce0814b40b5ac3633802b13ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing preference pairs:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization complete!\n",
            "Dataset keys: ['chosen', 'rejected', 'prompt', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']\n",
            "Sample chosen length: 512\n",
            "Sample rejected length: 512\n",
            "Final dataset size: 5000 examples\n"
          ]
        }
      ],
      "source": [
        "# Load and tokenize the preference dataset\n",
        "print(\"Loading and preparing preference dataset...\")\n",
        "\n",
        "# Load the preference dataset\n",
        "preference_dataset = load_dataset('json', data_files=dataset_path)['train']\n",
        "print(f\"Loaded {len(preference_dataset)} preference pairs\")\n",
        "\n",
        "# Load tokenizer from the SFT model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
        "\n",
        "# Critical: Set pad token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"✓ Pad token set to EOS token\")\n",
        "\n",
        "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "def tokenize_pairs(example):\n",
        "    \"\"\"\n",
        "    Tokenize chosen and rejected responses for RewardTrainer.\n",
        "    \n",
        "    Returns dictionary with keys expected by RewardTrainer:\n",
        "    - input_ids_chosen, attention_mask_chosen\n",
        "    - input_ids_rejected, attention_mask_rejected\n",
        "    \"\"\"\n",
        "    # Format the texts for reward modeling\n",
        "    chosen_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['chosen']}\"\n",
        "    rejected_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['rejected']}\"\n",
        "    \n",
        "    # Tokenize chosen response\n",
        "    chosen_tokens = tokenizer(\n",
        "        chosen_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    # Tokenize rejected response\n",
        "    rejected_tokens = tokenizer(\n",
        "        rejected_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\", \n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"],\n",
        "        \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"],\n",
        "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"],\n",
        "        \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# Apply tokenization to the entire dataset\n",
        "print(\"Tokenizing preference pairs...\")\n",
        "tokenized_dataset = preference_dataset.map(\n",
        "    tokenize_pairs,\n",
        "    batched=False,\n",
        "    desc=\"Tokenizing preference pairs\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Dataset keys: {list(tokenized_dataset[0].keys())}\")\n",
        "print(f\"Sample chosen length: {len(tokenized_dataset[0]['input_ids_chosen'])}\")\n",
        "print(f\"Sample rejected length: {len(tokenized_dataset[0]['input_ids_rejected'])}\")\n",
        "\n",
        "# Take a subset for faster training (optional)\n",
        "# tokenized_dataset = tokenized_dataset.select(range(min(1000, len(tokenized_dataset))))\n",
        "print(f\"Final dataset size: {len(tokenized_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing reward model factory...\n",
            "Creating bf16 reward model...\n",
            "  Using BF16 precision\n",
            "  Loading PEFT adapters...\n",
            "  ✅ PEFT adapters loaded and merged\n",
            "  Creating reward model with classification head...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Transformer weights copied successfully\n",
            "  ✅ bf16 reward model created successfully!\n",
            "✅ Factory function test successful! Model parameters: 81,913,344\n",
            "🧹 Test cleanup completed\n",
            "✅ Reward model factory function is ready!\n"
          ]
        }
      ],
      "source": [
        "# Reward Model Factory Function\n",
        "def create_reward_model(model_path, precision):\n",
        "    \"\"\"\n",
        "    Create a reward model with specified precision/quantization.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to the SFT model\n",
        "        precision: One of 'bf16', 'int8', 'int4'\n",
        "        \n",
        "    Returns:\n",
        "        reward_model: The reward model for training\n",
        "        tokenizer: Associated tokenizer\n",
        "    \"\"\"\n",
        "    print(f\"Creating {precision} reward model...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Define model loading kwargs based on precision\n",
        "    if precision == 'bf16':\n",
        "        print(\"  Using BF16 precision\")\n",
        "        model_kwargs = {\n",
        "            'torch_dtype': torch.bfloat16,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int8':\n",
        "        print(\"  Using 8-bit quantization\")\n",
        "        model_kwargs = {\n",
        "            'load_in_8bit': True,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int4':\n",
        "        print(\"  Using 4-bit QLoRA quantization\")\n",
        "        # Configure 4-bit quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",                # Use NormalFloat4 quantization\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,    # Compute in BF16\n",
        "            bnb_4bit_use_double_quant=True,           # Double quantization for better compression\n",
        "        )\n",
        "        model_kwargs = {\n",
        "            'quantization_config': bnb_config,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported precision: {precision}\")\n",
        "    \n",
        "    # Load the base model (this will be our SFT model)\n",
        "    try:\n",
        "        # First try to load as a PEFT model (if it has adapters)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            'distilgpt2',  # Base model name\n",
        "            **model_kwargs\n",
        "        )\n",
        "        \n",
        "        # Load PEFT adapters if they exist\n",
        "        if os.path.exists(os.path.join(model_path, 'adapter_config.json')):\n",
        "            print(\"  Loading PEFT adapters...\")\n",
        "            try:\n",
        "                model = PeftModel.from_pretrained(base_model, model_path)\n",
        "                # Merge adapters for reward modeling\n",
        "                model = model.merge_and_unload()\n",
        "                print(\"  ✅ PEFT adapters loaded and merged\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Could not load PEFT adapters: {e}\")\n",
        "                model = base_model\n",
        "        else:\n",
        "            print(\"  No PEFT adapters found, using base model\")\n",
        "            model = base_model\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading model: {e}\")\n",
        "        print(\"  Falling back to direct loading...\")\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                **model_kwargs\n",
        "            )\n",
        "        except:\n",
        "            print(\"  Using base model as final fallback\")\n",
        "            model = AutoModelForCausalLM.from_pretrained('distilgpt2', **model_kwargs)\n",
        "    \n",
        "    # Create reward model by adding a classification head\n",
        "    print(\"  Creating reward model with classification head...\")\n",
        "    try:\n",
        "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'distilgpt2',  # Use base model\n",
        "            num_labels=1,  # Single reward score\n",
        "            torch_dtype=torch.bfloat16 if precision == 'bf16' else None\n",
        "        )\n",
        "        \n",
        "        # Copy the transformer layers from our fine-tuned model\n",
        "        if hasattr(model, 'transformer') and hasattr(reward_model, 'transformer'):\n",
        "            try:\n",
        "                reward_model.transformer.load_state_dict(model.transformer.state_dict())\n",
        "                print(\"  ✅ Transformer weights copied successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Could not copy weights: {e}\")\n",
        "        elif hasattr(model, 'model') and hasattr(reward_model, 'model'):\n",
        "            try:\n",
        "                reward_model.model.load_state_dict(model.model.state_dict())\n",
        "                print(\"  ✅ Model weights copied successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Could not copy weights: {e}\")\n",
        "        else:\n",
        "            print(\"  ⚠️ Using base reward model without fine-tuned weights\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Error creating reward model: {e}\")\n",
        "        print(\"  Using alternative approach...\")\n",
        "        # Fallback: create a simple reward model\n",
        "        reward_model = model\n",
        "    \n",
        "    print(f\"  ✅ {precision} reward model created successfully!\")\n",
        "    return reward_model, tokenizer\n",
        "\n",
        "# 测试工厂函数\n",
        "print(\"Testing reward model factory...\")\n",
        "try:\n",
        "    test_model, test_tokenizer = create_reward_model(sft_model_path, 'bf16')\n",
        "    if test_model is not None:\n",
        "        print(f\"✅ Factory function test successful! Model parameters: {test_model.num_parameters():,}\")\n",
        "    else:\n",
        "        print(\"❌ Factory function returned None\")\n",
        "    \n",
        "    # 清理测试模型\n",
        "    del test_model, test_tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"🧹 Test cleanup completed\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Factory function test failed: {e}\")\n",
        "\n",
        "print(\"✅ Reward model factory function is ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting reward model training experiments...\n",
            "Will train 3 different precision models\n",
            "============================================================\n",
            "\n",
            "==================== Experiment 1/3 ====================\n",
            "--- Starting BF16 Reward Model Training ---\n",
            "Creating bf16 reward model...\n",
            "  Using BF16 precision\n",
            "  Loading PEFT adapters...\n",
            "  ✅ PEFT adapters loaded and merged\n",
            "  Creating reward model with classification head...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Transformer weights copied successfully\n",
            "  ✅ bf16 reward model created successfully!\n",
            "  Output directory: ./models/rm\\bf16\n",
            "  Model parameters: 81,913,344\n",
            "  Initializing RewardTrainer...\n",
            "  ❌ Error training bf16 model: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "  🧹 Cleaning up GPU memory after bf16 training...\n",
            "  Memory cleaned up for bf16 model\n",
            "\n",
            "==================== Experiment 2/3 ====================\n",
            "--- Starting INT8 Reward Model Training ---\n",
            "Creating int8 reward model...\n",
            "  Using 8-bit quantization\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loading PEFT adapters...\n",
            "  ✅ PEFT adapters loaded and merged\n",
            "  Creating reward model with classification head...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\downloads\\python\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:85: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⚠️ Could not copy weights: Error(s) in loading state_dict for GPT2Model:\n",
            "\tUnexpected key(s) in state_dict: \"h.0.attn.c_attn.SCB\", \"h.0.attn.c_attn.weight_format\", \"h.0.attn.c_proj.SCB\", \"h.0.attn.c_proj.weight_format\", \"h.0.mlp.c_fc.SCB\", \"h.0.mlp.c_fc.weight_format\", \"h.0.mlp.c_proj.SCB\", \"h.0.mlp.c_proj.weight_format\", \"h.1.attn.c_attn.SCB\", \"h.1.attn.c_attn.weight_format\", \"h.1.attn.c_proj.SCB\", \"h.1.attn.c_proj.weight_format\", \"h.1.mlp.c_fc.SCB\", \"h.1.mlp.c_fc.weight_format\", \"h.1.mlp.c_proj.SCB\", \"h.1.mlp.c_proj.weight_format\", \"h.2.attn.c_attn.SCB\", \"h.2.attn.c_attn.weight_format\", \"h.2.attn.c_proj.SCB\", \"h.2.attn.c_proj.weight_format\", \"h.2.mlp.c_fc.SCB\", \"h.2.mlp.c_fc.weight_format\", \"h.2.mlp.c_proj.SCB\", \"h.2.mlp.c_proj.weight_format\", \"h.3.attn.c_attn.SCB\", \"h.3.attn.c_attn.weight_format\", \"h.3.attn.c_proj.SCB\", \"h.3.attn.c_proj.weight_format\", \"h.3.mlp.c_fc.SCB\", \"h.3.mlp.c_fc.weight_format\", \"h.3.mlp.c_proj.SCB\", \"h.3.mlp.c_proj.weight_format\", \"h.4.attn.c_attn.SCB\", \"h.4.attn.c_attn.weight_format\", \"h.4.attn.c_proj.SCB\", \"h.4.attn.c_proj.weight_format\", \"h.4.mlp.c_fc.SCB\", \"h.4.mlp.c_fc.weight_format\", \"h.4.mlp.c_proj.SCB\", \"h.4.mlp.c_proj.weight_format\", \"h.5.attn.c_attn.SCB\", \"h.5.attn.c_attn.weight_format\", \"h.5.attn.c_proj.SCB\", \"h.5.attn.c_proj.weight_format\", \"h.5.mlp.c_fc.SCB\", \"h.5.mlp.c_fc.weight_format\", \"h.5.mlp.c_proj.SCB\", \"h.5.mlp.c_proj.weight_format\". \n",
            "\tsize mismatch for h.0.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.0.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.0.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.1.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.1.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.1.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.2.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.2.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.2.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.3.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.3.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.3.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.4.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.4.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.4.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.5.attn.c_attn.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.5.mlp.c_fc.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.5.mlp.c_proj.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "  ✅ int8 reward model created successfully!\n",
            "  Output directory: ./models/rm\\int8\n",
            "  Model parameters: 81,913,344\n",
            "  Initializing RewardTrainer...\n",
            "  ❌ Error training int8 model: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "  🧹 Cleaning up GPU memory after int8 training...\n",
            "  Memory cleaned up for int8 model\n",
            "\n",
            "==================== Experiment 3/3 ====================\n",
            "--- Starting INT4 Reward Model Training ---\n",
            "Creating int4 reward model...\n",
            "  Using 4-bit QLoRA quantization\n",
            "  Loading PEFT adapters...\n",
            "  ✅ PEFT adapters loaded and merged\n",
            "  Creating reward model with classification head...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\downloads\\python\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⚠️ Could not copy weights: Error(s) in loading state_dict for GPT2Model:\n",
            "\tUnexpected key(s) in state_dict: \"h.0.attn.c_attn.weight.absmax\", \"h.0.attn.c_attn.weight.quant_map\", \"h.0.attn.c_attn.weight.nested_absmax\", \"h.0.attn.c_attn.weight.nested_quant_map\", \"h.0.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.0.attn.c_proj.weight.absmax\", \"h.0.attn.c_proj.weight.quant_map\", \"h.0.attn.c_proj.weight.nested_absmax\", \"h.0.attn.c_proj.weight.nested_quant_map\", \"h.0.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.0.mlp.c_fc.weight.absmax\", \"h.0.mlp.c_fc.weight.quant_map\", \"h.0.mlp.c_fc.weight.nested_absmax\", \"h.0.mlp.c_fc.weight.nested_quant_map\", \"h.0.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.0.mlp.c_proj.weight.absmax\", \"h.0.mlp.c_proj.weight.quant_map\", \"h.0.mlp.c_proj.weight.nested_absmax\", \"h.0.mlp.c_proj.weight.nested_quant_map\", \"h.0.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.1.attn.c_attn.weight.absmax\", \"h.1.attn.c_attn.weight.quant_map\", \"h.1.attn.c_attn.weight.nested_absmax\", \"h.1.attn.c_attn.weight.nested_quant_map\", \"h.1.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.1.attn.c_proj.weight.absmax\", \"h.1.attn.c_proj.weight.quant_map\", \"h.1.attn.c_proj.weight.nested_absmax\", \"h.1.attn.c_proj.weight.nested_quant_map\", \"h.1.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.1.mlp.c_fc.weight.absmax\", \"h.1.mlp.c_fc.weight.quant_map\", \"h.1.mlp.c_fc.weight.nested_absmax\", \"h.1.mlp.c_fc.weight.nested_quant_map\", \"h.1.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.1.mlp.c_proj.weight.absmax\", \"h.1.mlp.c_proj.weight.quant_map\", \"h.1.mlp.c_proj.weight.nested_absmax\", \"h.1.mlp.c_proj.weight.nested_quant_map\", \"h.1.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.2.attn.c_attn.weight.absmax\", \"h.2.attn.c_attn.weight.quant_map\", \"h.2.attn.c_attn.weight.nested_absmax\", \"h.2.attn.c_attn.weight.nested_quant_map\", \"h.2.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.2.attn.c_proj.weight.absmax\", \"h.2.attn.c_proj.weight.quant_map\", \"h.2.attn.c_proj.weight.nested_absmax\", \"h.2.attn.c_proj.weight.nested_quant_map\", \"h.2.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.2.mlp.c_fc.weight.absmax\", \"h.2.mlp.c_fc.weight.quant_map\", \"h.2.mlp.c_fc.weight.nested_absmax\", \"h.2.mlp.c_fc.weight.nested_quant_map\", \"h.2.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.2.mlp.c_proj.weight.absmax\", \"h.2.mlp.c_proj.weight.quant_map\", \"h.2.mlp.c_proj.weight.nested_absmax\", \"h.2.mlp.c_proj.weight.nested_quant_map\", \"h.2.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.3.attn.c_attn.weight.absmax\", \"h.3.attn.c_attn.weight.quant_map\", \"h.3.attn.c_attn.weight.nested_absmax\", \"h.3.attn.c_attn.weight.nested_quant_map\", \"h.3.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.3.attn.c_proj.weight.absmax\", \"h.3.attn.c_proj.weight.quant_map\", \"h.3.attn.c_proj.weight.nested_absmax\", \"h.3.attn.c_proj.weight.nested_quant_map\", \"h.3.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.3.mlp.c_fc.weight.absmax\", \"h.3.mlp.c_fc.weight.quant_map\", \"h.3.mlp.c_fc.weight.nested_absmax\", \"h.3.mlp.c_fc.weight.nested_quant_map\", \"h.3.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.3.mlp.c_proj.weight.absmax\", \"h.3.mlp.c_proj.weight.quant_map\", \"h.3.mlp.c_proj.weight.nested_absmax\", \"h.3.mlp.c_proj.weight.nested_quant_map\", \"h.3.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.4.attn.c_attn.weight.absmax\", \"h.4.attn.c_attn.weight.quant_map\", \"h.4.attn.c_attn.weight.nested_absmax\", \"h.4.attn.c_attn.weight.nested_quant_map\", \"h.4.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.4.attn.c_proj.weight.absmax\", \"h.4.attn.c_proj.weight.quant_map\", \"h.4.attn.c_proj.weight.nested_absmax\", \"h.4.attn.c_proj.weight.nested_quant_map\", \"h.4.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.4.mlp.c_fc.weight.absmax\", \"h.4.mlp.c_fc.weight.quant_map\", \"h.4.mlp.c_fc.weight.nested_absmax\", \"h.4.mlp.c_fc.weight.nested_quant_map\", \"h.4.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.4.mlp.c_proj.weight.absmax\", \"h.4.mlp.c_proj.weight.quant_map\", \"h.4.mlp.c_proj.weight.nested_absmax\", \"h.4.mlp.c_proj.weight.nested_quant_map\", \"h.4.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.5.attn.c_attn.weight.absmax\", \"h.5.attn.c_attn.weight.quant_map\", \"h.5.attn.c_attn.weight.nested_absmax\", \"h.5.attn.c_attn.weight.nested_quant_map\", \"h.5.attn.c_attn.weight.quant_state.bitsandbytes__nf4\", \"h.5.attn.c_proj.weight.absmax\", \"h.5.attn.c_proj.weight.quant_map\", \"h.5.attn.c_proj.weight.nested_absmax\", \"h.5.attn.c_proj.weight.nested_quant_map\", \"h.5.attn.c_proj.weight.quant_state.bitsandbytes__nf4\", \"h.5.mlp.c_fc.weight.absmax\", \"h.5.mlp.c_fc.weight.quant_map\", \"h.5.mlp.c_fc.weight.nested_absmax\", \"h.5.mlp.c_fc.weight.nested_quant_map\", \"h.5.mlp.c_fc.weight.quant_state.bitsandbytes__nf4\", \"h.5.mlp.c_proj.weight.absmax\", \"h.5.mlp.c_proj.weight.quant_map\", \"h.5.mlp.c_proj.weight.nested_absmax\", \"h.5.mlp.c_proj.weight.nested_quant_map\", \"h.5.mlp.c_proj.weight.quant_state.bitsandbytes__nf4\". \n",
            "\tsize mismatch for h.0.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.0.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.0.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.0.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.1.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.1.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.1.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.1.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.2.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.2.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.2.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.2.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.3.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.3.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.3.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.3.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.4.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.4.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.4.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.4.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "\tsize mismatch for h.5.attn.c_attn.weight: copying a param with shape torch.Size([884736, 1]) from checkpoint, the shape in current model is torch.Size([768, 2304]).\n",
            "\tsize mismatch for h.5.attn.c_proj.weight: copying a param with shape torch.Size([294912, 1]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
            "\tsize mismatch for h.5.mlp.c_fc.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n",
            "\tsize mismatch for h.5.mlp.c_proj.weight: copying a param with shape torch.Size([1179648, 1]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n",
            "  ✅ int4 reward model created successfully!\n",
            "  Output directory: ./models/rm\\int4\n",
            "  Model parameters: 81,913,344\n",
            "  Initializing RewardTrainer...\n",
            "  ❌ Error training int4 model: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "  🧹 Cleaning up GPU memory after int4 training...\n",
            "  Memory cleaned up for int4 model\n",
            "\n",
            "============================================================\n",
            "🎉 All reward model experiments completed!\n",
            "\n",
            "📊 TRAINING SUMMARY:\n",
            "----------------------------------------\n",
            "❌ BF16   | Status: failed  \n",
            "        | Error: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "\n",
            "❌ INT8   | Status: failed  \n",
            "        | Error: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "\n",
            "❌ INT4   | Status: failed  \n",
            "        | Error: RewardTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
            "\n",
            "🎯 Your reward models are ready in: ./models/rm\n",
            "Next step: Use these models for PPO training or inference!\n"
          ]
        }
      ],
      "source": [
        "# Experiment Execution Loop - Train Multiple Reward Models\n",
        "print(\"🚀 Starting reward model training experiments...\")\n",
        "print(f\"Will train {len(precisions_to_run)} different precision models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Track results for summary\n",
        "training_results = {}\n",
        "\n",
        "for i, precision in enumerate(precisions_to_run):\n",
        "    print(f\"\\n{'='*20} Experiment {i+1}/{len(precisions_to_run)} {'='*20}\")\n",
        "    print(f\"--- Starting {precision.upper()} Reward Model Training ---\")\n",
        "    \n",
        "    try:\n",
        "        # Create reward model and tokenizer for current precision\n",
        "        model, tokenizer = create_reward_model(sft_model_path, precision)\n",
        "        \n",
        "        # Define output directory for this specific precision\n",
        "        output_dir = os.path.join(base_rm_output_dir, precision)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Create training arguments for this run\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            **shared_training_args  # Unpack shared arguments\n",
        "        )\n",
        "        \n",
        "        print(f\"  Output directory: {output_dir}\")\n",
        "        print(f\"  Model parameters: {model.num_parameters():,}\")\n",
        "        \n",
        "        # Initialize RewardTrainer\n",
        "        print(\"  Initializing RewardTrainer...\")\n",
        "        trainer = RewardTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            # eval_dataset=None,  # No evaluation for now\n",
        "        )\n",
        "        \n",
        "        print(f\"  Starting training for {precision} model...\")\n",
        "        # Start training\n",
        "        training_result = trainer.train()\n",
        "        \n",
        "        # Save the trained model\n",
        "        print(f\"  Saving {precision} model...\")\n",
        "        trainer.save_model()\n",
        "        \n",
        "        # Store results\n",
        "        training_results[precision] = {\n",
        "            'status': 'success',\n",
        "            'final_loss': training_result.training_loss if hasattr(training_result, 'training_loss') else 'N/A',\n",
        "            'output_dir': output_dir,\n",
        "            'model_size_mb': sum(os.path.getsize(os.path.join(output_dir, f)) \n",
        "                                for f in os.listdir(output_dir) \n",
        "                                if os.path.isfile(os.path.join(output_dir, f))) / (1024*1024)\n",
        "        }\n",
        "        \n",
        "        print(f\"  ✅ {precision.upper()} model training completed successfully!\")\n",
        "        print(f\"  Final loss: {training_results[precision]['final_loss']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error training {precision} model: {str(e)}\")\n",
        "        training_results[precision] = {\n",
        "            'status': 'failed',\n",
        "            'error': str(e),\n",
        "            'output_dir': output_dir if 'output_dir' in locals() else 'N/A'\n",
        "        }\n",
        "    \n",
        "    finally:\n",
        "        # CRITICAL: Clean up GPU memory before next iteration\n",
        "        # This prevents CUDA out of memory errors when switching between quantization levels\n",
        "        print(f\"  🧹 Cleaning up GPU memory after {precision} training...\")\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'trainer' in locals():\n",
        "            del trainer\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        \n",
        "        # Force garbage collection and clear CUDA cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"  Memory cleaned up for {precision} model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 All reward model experiments completed!\")\n",
        "\n",
        "# Print summary of results\n",
        "print(\"\\n📊 TRAINING SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "for precision, result in training_results.items():\n",
        "    status_emoji = \"✅\" if result['status'] == 'success' else \"❌\"\n",
        "    print(f\"{status_emoji} {precision.upper():6} | Status: {result['status']:8}\")\n",
        "    \n",
        "    if result['status'] == 'success':\n",
        "        print(f\"        | Loss: {result['final_loss']}\")\n",
        "        print(f\"        | Size: {result['model_size_mb']:.1f} MB\")\n",
        "        print(f\"        | Path: {result['output_dir']}\")\n",
        "    else:\n",
        "        print(f\"        | Error: {result['error']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"🎯 Your reward models are ready in: {base_rm_output_dir}\")\n",
        "print(\"Next step: Use these models for PPO training or inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
