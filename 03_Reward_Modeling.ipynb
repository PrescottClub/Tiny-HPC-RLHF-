{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Reward Modeling with Quantization\n",
        "\n",
        "This notebook serves as an experimental platform to train multiple reward models with different quantization levels (bf16, 8-bit, 4-bit) based on our fine-tuned SFT model. We'll systematically compare the performance and memory usage of different precision levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries for reward modeling\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM, \n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import RewardTrainer\n",
        "from peft import PeftModel\n",
        "import bitsandbytes as bnb\n",
        "import gc\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"BitsAndBytes version: {bnb.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Central Configuration for all reward modeling experiments\n",
        "print(\"Setting up experimental configuration...\")\n",
        "\n",
        "# Model and dataset paths\n",
        "sft_model_path = './models/sft'          # Path to our trained SFT adapters\n",
        "dataset_path = './data/train_prefs.jsonl'  # Preference dataset\n",
        "base_rm_output_dir = './models/rm'       # Base output directory for reward models\n",
        "\n",
        "# Precision levels to experiment with\n",
        "precisions_to_run = ['bf16', 'int8', 'int4']\n",
        "\n",
        "# Shared training arguments for all experiments\n",
        "shared_training_args = {\n",
        "    'per_device_train_batch_size': 4,     # Batch size per device\n",
        "    'gradient_accumulation_steps': 4,     # Effective batch size = 4*4 = 16\n",
        "    'num_train_epochs': 1,                # Number of training epochs\n",
        "    'learning_rate': 2e-4,                # Learning rate for reward model training\n",
        "    'logging_steps': 10,                  # Log every 10 steps\n",
        "    'bf16': True,                         # Use BF16 for training efficiency\n",
        "    'save_strategy': 'epoch',             # Save at the end of each epoch\n",
        "    'evaluation_strategy': 'no',          # No evaluation during training\n",
        "    'remove_unused_columns': False,       # Keep all columns\n",
        "    'push_to_hub': False,                 # Don't push to HF Hub\n",
        "    'report_to': None,                    # Disable logging to wandb/tensorboard\n",
        "    'dataloader_pin_memory': False,       # Reduce memory usage\n",
        "    'gradient_checkpointing': True,       # Trade compute for memory\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  SFT model path: {sft_model_path}\")\n",
        "print(f\"  Dataset path: {dataset_path}\")\n",
        "print(f\"  Base RM output dir: {base_rm_output_dir}\")\n",
        "print(f\"  Precisions to test: {precisions_to_run}\")\n",
        "print(f\"  Shared training args: {shared_training_args}\")\n",
        "\n",
        "# Ensure base output directory exists\n",
        "os.makedirs(base_rm_output_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and tokenize the preference dataset\n",
        "print(\"Loading and preparing preference dataset...\")\n",
        "\n",
        "# Load the preference dataset\n",
        "preference_dataset = load_dataset('json', data_files=dataset_path)['train']\n",
        "print(f\"Loaded {len(preference_dataset)} preference pairs\")\n",
        "\n",
        "# Load tokenizer from the SFT model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
        "\n",
        "# Critical: Set pad token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úì Pad token set to EOS token\")\n",
        "\n",
        "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "def tokenize_pairs(example):\n",
        "    \"\"\"\n",
        "    Tokenize chosen and rejected responses for RewardTrainer.\n",
        "    \n",
        "    Returns dictionary with keys expected by RewardTrainer:\n",
        "    - input_ids_chosen, attention_mask_chosen\n",
        "    - input_ids_rejected, attention_mask_rejected\n",
        "    \"\"\"\n",
        "    # Format the texts for reward modeling\n",
        "    chosen_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['chosen']}\"\n",
        "    rejected_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['rejected']}\"\n",
        "    \n",
        "    # Tokenize chosen response\n",
        "    chosen_tokens = tokenizer(\n",
        "        chosen_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    # Tokenize rejected response\n",
        "    rejected_tokens = tokenizer(\n",
        "        rejected_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\", \n",
        "        max_length=512,\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"],\n",
        "        \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"],\n",
        "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"],\n",
        "        \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "# Apply tokenization to the entire dataset\n",
        "print(\"Tokenizing preference pairs...\")\n",
        "tokenized_dataset = preference_dataset.map(\n",
        "    tokenize_pairs,\n",
        "    batched=False,\n",
        "    desc=\"Tokenizing preference pairs\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Dataset keys: {list(tokenized_dataset[0].keys())}\")\n",
        "print(f\"Sample chosen length: {len(tokenized_dataset[0]['input_ids_chosen'])}\")\n",
        "print(f\"Sample rejected length: {len(tokenized_dataset[0]['input_ids_rejected'])}\")\n",
        "\n",
        "# Take a subset for faster training (optional)\n",
        "# tokenized_dataset = tokenized_dataset.select(range(min(1000, len(tokenized_dataset))))\n",
        "print(f\"Final dataset size: {len(tokenized_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward Model Factory Function\n",
        "def create_reward_model(model_path, precision):\n",
        "    \"\"\"\n",
        "    Create a reward model with specified precision/quantization.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to the SFT model\n",
        "        precision: One of 'bf16', 'int8', 'int4'\n",
        "        \n",
        "    Returns:\n",
        "        reward_model: The reward model for training\n",
        "        tokenizer: Associated tokenizer\n",
        "    \"\"\"\n",
        "    print(f\"Creating {precision} reward model...\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Define model loading kwargs based on precision\n",
        "    if precision == 'bf16':\n",
        "        print(\"  Using BF16 precision\")\n",
        "        model_kwargs = {\n",
        "            'torch_dtype': torch.bfloat16,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int8':\n",
        "        print(\"  Using 8-bit quantization\")\n",
        "        model_kwargs = {\n",
        "            'load_in_8bit': True,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    elif precision == 'int4':\n",
        "        print(\"  Using 4-bit QLoRA quantization\")\n",
        "        # Configure 4-bit quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",                # Use NormalFloat4 quantization\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,    # Compute in BF16\n",
        "            bnb_4bit_use_double_quant=True,           # Double quantization for better compression\n",
        "        )\n",
        "        model_kwargs = {\n",
        "            'quantization_config': bnb_config,\n",
        "            'device_map': 'auto'\n",
        "        }\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported precision: {precision}\")\n",
        "    \n",
        "    # Load the base model (this will be our SFT model)\n",
        "    try:\n",
        "        # First try to load as a PEFT model (if it has adapters)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            'distilgpt2',  # Base model name\n",
        "            **model_kwargs\n",
        "        )\n",
        "        \n",
        "        # Load PEFT adapters if they exist\n",
        "        if os.path.exists(os.path.join(model_path, 'adapter_config.json')):\n",
        "            print(\"  Loading PEFT adapters...\")\n",
        "            model = PeftModel.from_pretrained(base_model, model_path)\n",
        "            # Merge adapters for reward modeling\n",
        "            model = model.merge_and_unload()\n",
        "        else:\n",
        "            print(\"  No PEFT adapters found, using base model\")\n",
        "            model = base_model\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading model: {e}\")\n",
        "        print(\"  Falling back to direct loading...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            **model_kwargs\n",
        "        )\n",
        "    \n",
        "    # Create reward model by adding a classification head\n",
        "    print(\"  Creating reward model with classification head...\")\n",
        "    reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        None,  # Don't load from pretrained\n",
        "        config=model.config,\n",
        "        num_labels=1,  # Single reward score\n",
        "        torch_dtype=model.dtype if hasattr(model, 'dtype') else torch.bfloat16\n",
        "    )\n",
        "    \n",
        "    # Copy the transformer layers from our fine-tuned model\n",
        "    if hasattr(model, 'transformer'):\n",
        "        reward_model.transformer = model.transformer\n",
        "    elif hasattr(model, 'model'):\n",
        "        reward_model.model = model.model\n",
        "    else:\n",
        "        # Copy all non-classifier parameters\n",
        "        for name, param in model.named_parameters():\n",
        "            if hasattr(reward_model, name.split('.')[0]):\n",
        "                target = reward_model\n",
        "                for attr in name.split('.'):\n",
        "                    if hasattr(target, attr):\n",
        "                        target = getattr(target, attr)\n",
        "                    else:\n",
        "                        break\n",
        "                else:\n",
        "                    target.data = param.data\n",
        "    \n",
        "    print(f\"  ‚úÖ {precision} reward model created successfully!\")\n",
        "    return reward_model, tokenizer\n",
        "\n",
        "# Test the factory function\n",
        "print(\"Testing reward model factory...\")\n",
        "test_model, test_tokenizer = create_reward_model(sft_model_path, 'bf16')\n",
        "print(f\"Test model parameters: {test_model.num_parameters():,}\")\n",
        "\n",
        "# Clean up test model\n",
        "del test_model, test_tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Factory function test complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment Execution Loop - Train Multiple Reward Models\n",
        "print(\"üöÄ Starting reward model training experiments...\")\n",
        "print(f\"Will train {len(precisions_to_run)} different precision models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Track results for summary\n",
        "training_results = {}\n",
        "\n",
        "for i, precision in enumerate(precisions_to_run):\n",
        "    print(f\"\\n{'='*20} Experiment {i+1}/{len(precisions_to_run)} {'='*20}\")\n",
        "    print(f\"--- Starting {precision.upper()} Reward Model Training ---\")\n",
        "    \n",
        "    try:\n",
        "        # Create reward model and tokenizer for current precision\n",
        "        model, tokenizer = create_reward_model(sft_model_path, precision)\n",
        "        \n",
        "        # Define output directory for this specific precision\n",
        "        output_dir = os.path.join(base_rm_output_dir, precision)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Create training arguments for this run\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            **shared_training_args  # Unpack shared arguments\n",
        "        )\n",
        "        \n",
        "        print(f\"  Output directory: {output_dir}\")\n",
        "        print(f\"  Model parameters: {model.num_parameters():,}\")\n",
        "        \n",
        "        # Initialize RewardTrainer\n",
        "        print(\"  Initializing RewardTrainer...\")\n",
        "        trainer = RewardTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            # eval_dataset=None,  # No evaluation for now\n",
        "        )\n",
        "        \n",
        "        print(f\"  Starting training for {precision} model...\")\n",
        "        # Start training\n",
        "        training_result = trainer.train()\n",
        "        \n",
        "        # Save the trained model\n",
        "        print(f\"  Saving {precision} model...\")\n",
        "        trainer.save_model()\n",
        "        \n",
        "        # Store results\n",
        "        training_results[precision] = {\n",
        "            'status': 'success',\n",
        "            'final_loss': training_result.training_loss if hasattr(training_result, 'training_loss') else 'N/A',\n",
        "            'output_dir': output_dir,\n",
        "            'model_size_mb': sum(os.path.getsize(os.path.join(output_dir, f)) \n",
        "                                for f in os.listdir(output_dir) \n",
        "                                if os.path.isfile(os.path.join(output_dir, f))) / (1024*1024)\n",
        "        }\n",
        "        \n",
        "        print(f\"  ‚úÖ {precision.upper()} model training completed successfully!\")\n",
        "        print(f\"  Final loss: {training_results[precision]['final_loss']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error training {precision} model: {str(e)}\")\n",
        "        training_results[precision] = {\n",
        "            'status': 'failed',\n",
        "            'error': str(e),\n",
        "            'output_dir': output_dir if 'output_dir' in locals() else 'N/A'\n",
        "        }\n",
        "    \n",
        "    finally:\n",
        "        # CRITICAL: Clean up GPU memory before next iteration\n",
        "        # This prevents CUDA out of memory errors when switching between quantization levels\n",
        "        print(f\"  üßπ Cleaning up GPU memory after {precision} training...\")\n",
        "        if 'model' in locals():\n",
        "            del model\n",
        "        if 'trainer' in locals():\n",
        "            del trainer\n",
        "        if 'tokenizer' in locals():\n",
        "            del tokenizer\n",
        "        \n",
        "        # Force garbage collection and clear CUDA cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"  Memory cleaned up for {precision} model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ All reward model experiments completed!\")\n",
        "\n",
        "# Print summary of results\n",
        "print(\"\\nüìä TRAINING SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "for precision, result in training_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {precision.upper():6} | Status: {result['status']:8}\")\n",
        "    \n",
        "    if result['status'] == 'success':\n",
        "        print(f\"        | Loss: {result['final_loss']}\")\n",
        "        print(f\"        | Size: {result['model_size_mb']:.1f} MB\")\n",
        "        print(f\"        | Path: {result['output_dir']}\")\n",
        "    else:\n",
        "        print(f\"        | Error: {result['error']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"üéØ Your reward models are ready in: {base_rm_output_dir}\")\n",
        "print(\"Next step: Use these models for PPO training or inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
