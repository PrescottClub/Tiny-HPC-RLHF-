{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. Supervised Fine-Tuning (SFT) with LoRA\n",
        "\n",
        "This notebook performs supervised fine-tuning on our prepared dataset using Parameter-Efficient Fine-Tuning (PEFT) with LoRA. We'll optimize the training for RTX 4060 with 8GB VRAM to achieve efficient and effective fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SFT Configuration variables\n",
        "# Start with a small and fast model for the first run\n",
        "# This can be swapped later for a larger model like 'gpt2-medium' or 'microsoft/DialoGPT-medium'\n",
        "base_model_name = 'distilgpt2'\n",
        "\n",
        "# Dataset and output paths\n",
        "dataset_path = './data/train_prefs.jsonl'\n",
        "sft_output_dir = './models/sft'\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"Base model: {base_model_name}\")\n",
        "print(f\"Dataset path: {dataset_path}\")\n",
        "print(f\"SFT output directory: {sft_output_dir}\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(sft_output_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and format the dataset for SFT training\n",
        "print(\"Loading dataset for SFT...\")\n",
        "\n",
        "# Load the prepared dataset\n",
        "train_dataset = load_dataset('json', data_files=dataset_path)['train']\n",
        "\n",
        "print(f\"Dataset loaded with {len(train_dataset)} examples\")\n",
        "print(f\"Original keys: {list(train_dataset[0].keys())}\")\n",
        "\n",
        "def format_example(example):\n",
        "    \"\"\"\n",
        "    Format example for SFT training by combining prompt and chosen response\n",
        "    into a single text field with proper formatting.\n",
        "    \"\"\"\n",
        "    formatted_text = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n{example['chosen']}\"\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# Apply formatting to the entire dataset\n",
        "print(\"\\nFormatting dataset...\")\n",
        "formatted_dataset = train_dataset.map(format_example)\n",
        "\n",
        "print(f\"Formatted dataset keys: {list(formatted_dataset[0].keys())}\")\n",
        "print(f\"\\nExample formatted text (first 300 chars):\")\n",
        "print(formatted_dataset[0]['text'][:300] + \"...\")\n",
        "\n",
        "print(f\"\\nDataset ready for SFT training with {len(formatted_dataset)} examples!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model and tokenizer\n",
        "print(f\"Loading tokenizer and model: {base_model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Critical step: Check and set pad token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"✓ Pad token set to EOS token\")\n",
        "else:\n",
        "    print(\"✓ Pad token already exists\")\n",
        "\n",
        "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# Load the base model\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,  # Use half precision for memory efficiency\n",
        "    device_map=\"auto\"  # Automatically place model on available GPU\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Print model architecture summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA (Parameter-Efficient Fine-Tuning)\n",
        "print(\"Configuring LoRA for efficient fine-tuning...\")\n",
        "\n",
        "# Create LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # Rank of the update matrices (higher = more parameters)\n",
        "    lora_alpha=32,           # LoRA scaling factor (usually 2*r)\n",
        "    lora_dropout=0.05,       # Dropout rate for LoRA layers\n",
        "    bias=\"none\",             # No bias updates\n",
        "    task_type=\"CAUSAL_LM\"    # Task type for causal language modeling\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  Bias: {lora_config.bias}\")\n",
        "print(f\"  Task type: {lora_config.task_type}\")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters after applying LoRA\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"✓ LoRA configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training arguments optimized for RTX 4060\n",
        "print(\"Configuring training arguments for RTX 4060...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=sft_output_dir,\n",
        "    per_device_train_batch_size=2,        # Small batch size for 8GB VRAM\n",
        "    gradient_accumulation_steps=8,         # Effective batch size = 2*8 = 16\n",
        "    learning_rate=2e-4,                    # Learning rate for LoRA\n",
        "    logging_steps=20,                      # Log every 20 steps\n",
        "    num_train_epochs=1,                    # Number of training epochs\n",
        "    max_steps=-1,                          # Train for epochs, not steps\n",
        "    bf16=True,                             # Essential for RTX 4060 performance\n",
        "    save_strategy=\"epoch\",                 # Save at the end of each epoch\n",
        "    evaluation_strategy=\"no\",              # No evaluation during training\n",
        "    remove_unused_columns=False,           # Keep all dataset columns\n",
        "    push_to_hub=False,                     # Don't push to Hugging Face Hub\n",
        "    report_to=None,                        # Disable wandb/tensorboard logging\n",
        "    dataloader_pin_memory=False,           # Reduce memory usage\n",
        "    gradient_checkpointing=True,           # Trade compute for memory\n",
        ")\n",
        "\n",
        "print(\"Training Arguments:\")\n",
        "print(f\"  Batch size per device: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  BF16 enabled: {training_args.bf16}\")\n",
        "\n",
        "# Instantiate the SFTTrainer\n",
        "print(\"\\nInitializing SFTTrainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",             # The column containing our formatted text\n",
        "    max_seq_length=512,                    # Maximum sequence length\n",
        "    packing=False,                         # Don't pack multiple samples\n",
        ")\n",
        "\n",
        "print(\"✅ SFTTrainer initialized successfully!\")\n",
        "print(f\"Total training steps: {len(formatted_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training and save the model\n",
        "print(\"🚀 Starting SFT training...\")\n",
        "print(\"This may take some time depending on your dataset size and hardware.\")\n",
        "print(\"You can monitor the progress through the logging output below.\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"🎉 SFT training finished!\")\n",
        "\n",
        "# Save the LoRA adapters\n",
        "print(\"\\nSaving the trained model...\")\n",
        "trainer.save_model()\n",
        "\n",
        "print(f\"✅ Model saved successfully to: {sft_output_dir}\")\n",
        "print(\"\\nThe following files have been created:\")\n",
        "print(\"  📁 adapter_config.json - LoRA configuration\")\n",
        "print(\"  📁 adapter_model.safetensors - Trained LoRA weights\")\n",
        "print(\"  📁 training_args.bin - Training arguments\")\n",
        "\n",
        "# Verify saved files\n",
        "saved_files = os.listdir(sft_output_dir)\n",
        "print(f\"\\nActual files in {sft_output_dir}:\")\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(sft_output_dir, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "    print(f\"  📄 {file} ({file_size:.2f} MB)\")\n",
        "\n",
        "print(f\"\\n🎯 Your LoRA adapters are ready! These small files contain all the knowledge\")\n",
        "print(f\"needed to make the base model {base_model_name} follow your customer service style.\")\n",
        "print(f\"\\nNext step: Use these adapters in reward model training or inference!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
