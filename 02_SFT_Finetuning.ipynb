{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. Supervised Fine-Tuning (SFT) with LoRA\n",
        "\n",
        "This notebook performs supervised fine-tuning on our prepared dataset using Parameter-Efficient Fine-Tuning (PEFT) with LoRA. We'll optimize the training for RTX 4060 with 8GB VRAM to achieve efficient and effective fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\downloads\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "All libraries imported successfully!\n",
            "PyTorch version: 2.7.0+cu118\n",
            "Transformers version: 4.53.0\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "Base model: distilgpt2\n",
            "Dataset path: ./data/train_prefs.jsonl\n",
            "SFT output directory: ./models/sft\n"
          ]
        }
      ],
      "source": [
        "# SFT Configuration variables\n",
        "# Start with a small and fast model for the first run\n",
        "# This can be swapped later for a larger model like 'gpt2-medium' or 'microsoft/DialoGPT-medium'\n",
        "base_model_name = 'distilgpt2'\n",
        "\n",
        "# Dataset and output paths\n",
        "dataset_path = './data/train_prefs.jsonl'\n",
        "sft_output_dir = './models/sft'\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"Base model: {base_model_name}\")\n",
        "print(f\"Dataset path: {dataset_path}\")\n",
        "print(f\"SFT output directory: {sft_output_dir}\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(sft_output_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset for SFT...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0841207fec8f4f4a9a9be830eeea6752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 5000 examples\n",
            "Original keys: ['chosen', 'rejected', 'prompt']\n",
            "\n",
            "Formatting dataset for TRL 0.19.0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "538d414d446346508e0bf212758c36ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted dataset keys: ['chosen', 'rejected', 'prompt', 'completion']\n",
            "\n",
            "Example formatted prompt (first 200 chars):\n",
            "### Human:\n",
            "Human: Why did cells originally combine together to create life?\n",
            "\n",
            "### Assistant:\n",
            "...\n",
            "\n",
            "Example completion (first 200 chars):\n",
            "Because their simple components -- chemicals -- interacted in particular ways.  And because of chemical processes involving acids and bases, certain kinds of chemicals can begin to self-organize into ...\n",
            "\n",
            "Dataset ready for SFT training with 5000 examples!\n"
          ]
        }
      ],
      "source": [
        "# Load and format the dataset for SFT training (TRL 0.19.0 compatible)\n",
        "print(\"Loading dataset for SFT...\")\n",
        "\n",
        "# Load the prepared dataset\n",
        "train_dataset = load_dataset('json', data_files=dataset_path)['train']\n",
        "\n",
        "print(f\"Dataset loaded with {len(train_dataset)} examples\")\n",
        "print(f\"Original keys: {list(train_dataset[0].keys())}\")\n",
        "\n",
        "def format_example(example):\n",
        "    \"\"\"\n",
        "    Format example for SFT training with TRL 0.19.0 compatible format.\n",
        "    TRL 0.19.0 expects separate 'prompt' and 'completion' fields.\n",
        "    \"\"\"\n",
        "    # Format prompt with instruction template\n",
        "    prompt = f\"### Human:\\n{example['prompt']}\\n\\n### Assistant:\\n\"\n",
        "    completion = example['chosen']\n",
        "    \n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"completion\": completion\n",
        "    }\n",
        "\n",
        "# Apply formatting to the entire dataset\n",
        "print(\"\\nFormatting dataset for TRL 0.19.0...\")\n",
        "formatted_dataset = train_dataset.map(format_example)\n",
        "\n",
        "print(f\"Formatted dataset keys: {list(formatted_dataset[0].keys())}\")\n",
        "print(f\"\\nExample formatted prompt (first 200 chars):\")\n",
        "print(formatted_dataset[0]['prompt'][:200] + \"...\")\n",
        "print(f\"\\nExample completion (first 200 chars):\")\n",
        "print(formatted_dataset[0]['completion'][:200] + \"...\")\n",
        "\n",
        "print(f\"\\nDataset ready for SFT training with {len(formatted_dataset)} examples!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer and model: distilgpt2\n",
            "✓ Pad token set to EOS token\n",
            "Tokenizer loaded - Vocab size: 50257\n",
            "Loading base model...\n",
            "✓ Model loaded successfully!\n",
            "Model parameters: 81,912,576\n",
            "Model device: cuda:0\n",
            "Total parameters: 81,912,576\n",
            "Trainable parameters: 81,912,576\n"
          ]
        }
      ],
      "source": [
        "# Load base model and tokenizer\n",
        "print(f\"Loading tokenizer and model: {base_model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Critical step: Check and set pad token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"✓ Pad token set to EOS token\")\n",
        "else:\n",
        "    print(\"✓ Pad token already exists\")\n",
        "\n",
        "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# Load the base model\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for better training stability and RTX 4060 compatibility\n",
        "    device_map=\"auto\"  # Automatically place model on available GPU\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded successfully!\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Print model architecture summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA for efficient fine-tuning...\n",
            "LoRA Configuration:\n",
            "  Rank (r): 16\n",
            "  Alpha: 32\n",
            "  Dropout: 0.05\n",
            "  Bias: none\n",
            "  Task type: CAUSAL_LM\n",
            "trainable params: 294,912 || all params: 82,207,488 || trainable%: 0.3587\n",
            "✓ LoRA configuration complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\downloads\\python\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA (Parameter-Efficient Fine-Tuning)\n",
        "print(\"Configuring LoRA for efficient fine-tuning...\")\n",
        "\n",
        "# Create LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                    # Rank of the update matrices (higher = more parameters)\n",
        "    lora_alpha=32,           # LoRA scaling factor (usually 2*r)\n",
        "    lora_dropout=0.05,       # Dropout rate for LoRA layers\n",
        "    bias=\"none\",             # No bias updates\n",
        "    task_type=\"CAUSAL_LM\"    # Task type for causal language modeling\n",
        ")\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(f\"  Rank (r): {lora_config.r}\")\n",
        "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  Bias: {lora_config.bias}\")\n",
        "print(f\"  Task type: {lora_config.task_type}\")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters after applying LoRA\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"✓ LoRA configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring training arguments for RTX 4060...\n",
            "Training Arguments:\n",
            "  Batch size per device: 1\n",
            "  Gradient accumulation steps: 8\n",
            "  Effective batch size: 8\n",
            "  Learning rate: 0.0002\n",
            "  Number of epochs: 1\n",
            "  BF16 enabled: True\n",
            "\n",
            "Converting dataset format for stable training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebfd1733f839493e8ded11f91477147b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text dataset ready with 5000 examples\n",
            "Sample text (first 200 chars): Human: ### Human:\n",
            "Human: Why did cells originally combine together to create life?\n",
            "\n",
            "### Assistant:\n",
            "\n",
            "\n",
            "Assistant: Because their simple components -- chemicals -- interacted in particular ways.  And beca...\n",
            "\n",
            "Initializing SFTTrainer with corrected parameters...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f5652ec89804275a6c6a89f5415290b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "803ae71c5c1a4cd7b0af9ff6223dc07c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1875 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16bdf8713fc244ec913903e3f64047d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ SFTTrainer initialized successfully!\n",
            "Total training steps: 625\n",
            "Estimated training time: 1875 seconds (approximate)\n"
          ]
        }
      ],
      "source": [
        "# Configure training arguments optimized for RTX 4060\n",
        "print(\"Configuring training arguments for RTX 4060...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=sft_output_dir,\n",
        "    per_device_train_batch_size=1,        # Very small batch size for 8GB VRAM\n",
        "    gradient_accumulation_steps=8,         # Effective batch size = 1*8 = 8\n",
        "    learning_rate=2e-4,                    # Learning rate for LoRA\n",
        "    logging_steps=20,                      # Log every 20 steps\n",
        "    num_train_epochs=1,                    # Number of training epochs\n",
        "    max_steps=-1,                          # Train for epochs, not steps\n",
        "    bf16=True,                             # Essential for RTX 4060 performance\n",
        "    save_strategy=\"epoch\",                 # Save at the end of each epoch\n",
        "    eval_strategy=\"no\",                    # Fixed: Changed from evaluation_strategy to eval_strategy\n",
        "    remove_unused_columns=False,           # Keep all dataset columns\n",
        "    push_to_hub=False,                     # Don't push to Hugging Face Hub\n",
        "    report_to=None,                        # Disable wandb/tensorboard logging\n",
        "    dataloader_pin_memory=False,           # Reduce memory usage\n",
        "    gradient_checkpointing=True,           # Trade compute for memory\n",
        "    warmup_steps=10,                       # Add warmup steps\n",
        "    weight_decay=0.01,                     # Small weight decay\n",
        ")\n",
        "\n",
        "print(\"Training Arguments:\")\n",
        "print(f\"  Batch size per device: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  BF16 enabled: {training_args.bf16}\")\n",
        "\n",
        "# Convert dataset back to simple text format for compatibility\n",
        "print(\"\\nConverting dataset format for stable training...\")\n",
        "\n",
        "def convert_to_text_format(example):\n",
        "    \"\"\"Convert to simple text format that works reliably\"\"\"\n",
        "    text = f\"Human: {example['prompt']}\\n\\nAssistant: {example['chosen']}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply conversion and remove original columns\n",
        "text_dataset = formatted_dataset.map(convert_to_text_format, \n",
        "                                    remove_columns=formatted_dataset.column_names)\n",
        "\n",
        "print(f\"Text dataset ready with {len(text_dataset)} examples\")\n",
        "print(f\"Sample text (first 200 chars): {text_dataset[0]['text'][:200]}...\")\n",
        "\n",
        "# Instantiate the SFTTrainer with corrected configuration\n",
        "print(\"\\nInitializing SFTTrainer with corrected parameters...\")\n",
        "\n",
        "try:\n",
        "    # 完全重置Accelerator状态\n",
        "    import torch\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.state import AcceleratorState\n",
        "    \n",
        "    # 清除共享状态\n",
        "    if hasattr(AcceleratorState, '_shared_state') and AcceleratorState._shared_state:\n",
        "        AcceleratorState._shared_state.clear()\n",
        "    \n",
        "    # 创建新的加速器实例\n",
        "    accelerator = Accelerator()\n",
        "    \n",
        "    # 正确的SFTTrainer初始化（移除所有不支持的参数）\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=text_dataset,\n",
        "        # 只保留基本参数，移除所有可能导致问题的参数\n",
        "    )\n",
        "    \n",
        "    print(\"✅ SFTTrainer initialized successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ SFTTrainer initialization failed: {e}\")\n",
        "    \n",
        "    # Try with even more minimal configuration\n",
        "    print(\"Trying with minimal TRL-compatible configuration...\")\n",
        "    try:\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=text_dataset,\n",
        "        )\n",
        "        print(\"✅ SFTTrainer initialized with minimal config!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"❌ Minimal config also failed: {e2}\")\n",
        "        print(\"The issue might be with TRL version compatibility.\")\n",
        "\n",
        "# Calculate training steps\n",
        "total_steps = len(text_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Estimated training time: {total_steps * 3} seconds (approximate)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting SFT training...\n",
            "This may take some time depending on your dataset size and hardware.\n",
            "You can monitor the progress through the logging output below.\n",
            "------------------------------------------------------------\n",
            "Resetting accelerator state for clean training...\n",
            "✅ Accelerator state reset successfully\n",
            "Pre-training verification:\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "Has trainable parameters: True\n",
            "Attempting to start training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "d:\\downloads\\python\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Gradient flow error detected.\n",
            "🔧 Trying to fix gradient requirements...\n",
            "Attempting training again with fixed gradients...\n",
            "❌ Training still failed: element 0 of tensors does not require grad and does not have a grad_fn\n",
            "\n",
            "Attempting to save the trained model...\n",
            "✅ Model saved to: ./models/sft\n",
            "\n",
            "Files in ./models/sft:\n",
            "  📄 adapter_config.json (0.00 MB)\n",
            "  📄 adapter_model.safetensors (1.13 MB)\n",
            "  📄 merges.txt (0.44 MB)\n",
            "  📄 README.md (0.00 MB)\n",
            "  📄 special_tokens_map.json (0.00 MB)\n",
            "  📄 tokenizer.json (3.39 MB)\n",
            "  📄 tokenizer_config.json (0.00 MB)\n",
            "  📄 training_args.bin (0.01 MB)\n",
            "  📄 vocab.json (0.76 MB)\n",
            "\n",
            "🎯 Total size: 5.73 MB\n",
            "✅ LoRA adapters saved successfully!\n",
            "These adapters can be used for inference or further training.\n",
            "\n",
            "📝 Summary:\n",
            "Base model: distilgpt2\n",
            "Training attempted on 5000 examples\n",
            "Output directory: ./models/sft\n",
            "\n",
            "🔄 Training process completed (check messages above for success/failure status)\n"
          ]
        }
      ],
      "source": [
        "# Start training and save the model\n",
        "print(\"🚀 Starting SFT training...\")\n",
        "print(\"This may take some time depending on your dataset size and hardware.\")\n",
        "print(\"You can monitor the progress through the logging output below.\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Reset accelerator state to fix potential state issues\n",
        "print(\"Resetting accelerator state for clean training...\")\n",
        "try:\n",
        "    from accelerate import Accelerator\n",
        "    # Create a new accelerator instance to reset state\n",
        "    accelerator = Accelerator()\n",
        "    print(\"✅ Accelerator state reset successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not reset accelerator: {e}\")\n",
        "    print(\"Continuing without accelerator reset...\")\n",
        "\n",
        "# Pre-training verification\n",
        "print(\"Pre-training verification:\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "print(f\"Has trainable parameters: {any(p.requires_grad for p in model.parameters())}\")\n",
        "\n",
        "# Ensure model is in training mode\n",
        "model.train()\n",
        "\n",
        "# Start the training process with comprehensive error handling\n",
        "try:\n",
        "    print(\"Attempting to start training...\")\n",
        "    training_output = trainer.train()\n",
        "    print(\"-\" * 60)\n",
        "    print(\"🎉 SFT training finished successfully!\")\n",
        "    \n",
        "    # Display training statistics if available\n",
        "    if hasattr(training_output, 'training_loss'):\n",
        "        print(f\"Final training loss: {training_output.training_loss:.4f}\")\n",
        "    \n",
        "except RuntimeError as e:\n",
        "    error_msg = str(e).lower()\n",
        "    if \"does not require grad\" in error_msg:\n",
        "        print(\"❌ Gradient flow error detected.\")\n",
        "        print(\"🔧 Trying to fix gradient requirements...\")\n",
        "        \n",
        "        # Try to enable gradients for LoRA parameters\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'lora' in name.lower() or param.requires_grad:\n",
        "                param.requires_grad_(True)\n",
        "        \n",
        "        print(\"Attempting training again with fixed gradients...\")\n",
        "        try:\n",
        "            training_output = trainer.train()\n",
        "            print(\"✅ Training successful after gradient fix!\")\n",
        "        except Exception as e2:\n",
        "            print(f\"❌ Training still failed: {e2}\")\n",
        "            \n",
        "    elif \"acceleratorstate\" in error_msg or \"distributed_type\" in error_msg:\n",
        "        print(\"❌ Accelerator state error detected.\")\n",
        "        print(\"🔧 Suggested solutions:\")\n",
        "        print(\"1. Restart the notebook kernel\")\n",
        "        print(\"2. Try: pip install --upgrade accelerate transformers\")\n",
        "        print(\"3. This is a known issue with certain TRL/Accelerate versions\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"❌ Training failed with error: {e}\")\n",
        "        print(\"🔧 Suggested solutions:\")\n",
        "        print(\"1. Try reducing batch size or sequence length\")\n",
        "        print(\"2. Check GPU memory availability\")\n",
        "        print(\"3. Consider using different TRL version\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected training error: {e}\")\n",
        "    print(\"The training encountered an unexpected issue.\")\n",
        "\n",
        "# Try to save the LoRA adapters regardless\n",
        "print(\"\\nAttempting to save the trained model...\")\n",
        "try:\n",
        "    trainer.save_model()\n",
        "    print(f\"✅ Model saved to: {sft_output_dir}\")\n",
        "    \n",
        "    # Verify saved files\n",
        "    if os.path.exists(sft_output_dir):\n",
        "        saved_files = os.listdir(sft_output_dir)\n",
        "        if saved_files:\n",
        "            print(f\"\\nFiles in {sft_output_dir}:\")\n",
        "            total_size = 0\n",
        "            for file in saved_files:\n",
        "                file_path = os.path.join(sft_output_dir, file)\n",
        "                if os.path.isfile(file_path):\n",
        "                    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "                    total_size += file_size\n",
        "                    print(f\"  📄 {file} ({file_size:.2f} MB)\")\n",
        "            \n",
        "            print(f\"\\n🎯 Total size: {total_size:.2f} MB\")\n",
        "            \n",
        "            if 'adapter_model.safetensors' in saved_files:\n",
        "                print(\"✅ LoRA adapters saved successfully!\")\n",
        "                print(\"These adapters can be used for inference or further training.\")\n",
        "            else:\n",
        "                print(\"⚠️  adapter_model.safetensors not found - training may not have completed\")\n",
        "        else:\n",
        "            print(\"❌ No files found in output directory\")\n",
        "    else:\n",
        "        print(\"❌ Output directory does not exist\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not save model: {e}\")\n",
        "\n",
        "print(f\"\\n📝 Summary:\")\n",
        "print(f\"Base model: {base_model_name}\")\n",
        "print(f\"Training attempted on {len(text_dataset)} examples\")\n",
        "print(f\"Output directory: {sft_output_dir}\")\n",
        "print(\"\\n🔄 Training process completed (check messages above for success/failure status)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
